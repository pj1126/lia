# -*- coding: utf-8 -*-
"""LIA_agency_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xAJ6NeXAO_2F4rqCsLniTBcv_b8-dtKX
"""

import pandas as pd
df = pd.read_csv('/content/CLSdata.csv', sep=',')
df.head()

df.info()

df.shape
# There are 16936 data with 58 variables

# convert the columns to datetime format
df['ENTRY_DATE']= pd.to_datetime(df['ENTRY_DATE'])
df['ISSUE_DATE']= pd.to_datetime(df['ISSUE_DATE'])
df['AGENT_JOIN_DATE']= pd.to_datetime(df['AGENT_JOIN_DATE'])
df['ALTERATION_DATE']= pd.to_datetime(df['ALTERATION_DATE'])

# Check the format of 'Date' column
df.info()

# Add in extra column into df, agent join year & cases

df['AGENT_JOIN_YEAR'] = df['AGENT_JOIN_DATE'].dt.strftime('%Y')

#check on join year max & min, most agent joined which year
print('Max join year =', df.AGENT_JOIN_YEAR.max())
print('Min join year =', df.AGENT_JOIN_YEAR.min())
print('Most cases are submitted from agent joined in =', df.AGENT_JOIN_YEAR.mode())

df['Cases'] = 1

# Cleaned data frame with only Issued cases

df2 = df.dropna(subset=['ISSUE_DATE'])
df2.shape

# Add in one column Issue_Year
df2['ISSUE_YEAR'] = df2['ISSUE_DATE'].dt.strftime('%Y')
df2.sample(3)

# Check df2 issue year range
print('min issue year =', df2.ISSUE_YEAR.min())
print('max issue year =', df2.ISSUE_YEAR.max())

# Cleaned data frame with only Issued cases in 2021 & 2022

df3 = df2[df2['ISSUE_YEAR'] != '2023']

# double check data
print('max issue year =', df3.ISSUE_YEAR.max())

# Rename the column
df4 = df3.rename(columns = {" AFYP regular ":"AFYP_Regular", " AFYP exclude reg. top up ":"AFYP_exclude_RTU"} )

df4.shape

df4.info()

# Extract column that is useful, save as df5
df5 = df4.loc[:, ['PRODUCT_NAME','AFYP_Regular', 'AFYP_exclude_RTU','POLY_STATUS', 'POLICYHOLDER NAME', 'SUM_ASSURED', 'PAYMENT_FREQUENCY' ,'PAYMENT_METHOD', 'REGION_NAME', 'BRANCH_NAME', 'AGENT_NAME', 'AM_NAME', 'GAM_NAME', 'AGENT_JOIN_YEAR', 'ISSUE_YEAR','ROP','Cases']]
df5.dtypes

df5_mean = df5.mean()
print(df5_mean)

# Change object to float, by setting errors=’coerce’, you’ll transform the non-numeric values into NaN
import numpy as np

df5['AFYP_Regular'] = pd.to_numeric(df5['AFYP_Regular'], errors='coerce')
df5['AFYP_exclude_RTU'] = pd.to_numeric(df5['AFYP_exclude_RTU'], errors='coerce')
df5['AGENT_JOIN_YEAR'] = pd.to_numeric(df5['AGENT_JOIN_YEAR'], errors='coerce')
df5['ISSUE_YEAR'] = pd.to_numeric(df5['ISSUE_YEAR'], errors='coerce')
df5['ROP'] = np.where(df5['ROP'] > 'ROP', 1, 0)
print(df5.dtypes)

# check columns with NaN in dataframe
print('rows with NaN =', df5.isna().any().sum())

# Drop rows with NaN, store in df6

df6 = df5.dropna()
df6.shape

df6.head()

print("Min for df6 =", df6['AFYP_Regular'].min())
print("Max for df6 =", df6['AFYP_Regular'].max())
print("Average for df6 =", df6['AFYP_Regular'].mean())

# check columns with NaN in dataframe >> no missing data
print('rows with NaN =', df6.isna().any().sum())

# Add 4 column on product name

df6['Securelink'] = df6.loc[:, 'PRODUCT_NAME']
df6['Maxipro'] = df6.loc[:, 'PRODUCT_NAME']
df6['Megaplus'] = df6.loc[:, 'PRODUCT_NAME']
df6['Others'] = df6.loc[:, 'PRODUCT_NAME']

df6.head(5)

# Replace the columns for the product above

df6['Securelink'] = df6['Securelink'].replace(['SecureLink', 'MaxiPro','MegaPlus','Agency Provident Fund','Aspire','Enrich Life Plan','Etiqa Life Secure','Protect 88','Triple Growth'], ['1','0','0','0','0','0','0','0','0'])
df6['Maxipro'] = df6['Maxipro'].replace(['SecureLink', 'MaxiPro','MegaPlus','Agency Provident Fund','Aspire','Enrich Life Plan','Etiqa Life Secure','Protect 88','Triple Growth'], ['0','1','0','0','0','0','0','0','0'])
df6['Megaplus'] = df6['Megaplus'].replace(['SecureLink', 'MaxiPro','MegaPlus','Agency Provident Fund','Aspire','Enrich Life Plan','Etiqa Life Secure','Protect 88','Triple Growth'], ['0','0','1','0','0','0','0','0','0'])
df6['Others'] = df6['Others'].replace(['SecureLink', 'MaxiPro','MegaPlus','Agency Provident Fund','Aspire','Enrich Life Plan','Etiqa Life Secure','Protect 88','Triple Growth'], ['0','0','0','1','1','1','1','1','1'])

df6.head(5)

df6.shape

# change object into float

df6['Securelink'] = pd.to_numeric(df6['Securelink'], errors='coerce')
df6['Megaplus'] = pd.to_numeric(df6['Megaplus'], errors='coerce')
df6['Maxipro'] = pd.to_numeric(df6['Maxipro'], errors='coerce')
df6['Others'] = pd.to_numeric(df6['Others'], errors='coerce')

df6.dtypes

"""✅ **Use only data from FY2022, group data into agents. Store as ag22**"""

# Extract 2022 data

ag22 = df6.loc[df6.ISSUE_YEAR == 2022, :]
print(ag22.shape[0])

# Groupby agent name & issue date, Year 2022
ag22_group = ag22.groupby(['AGENT_NAME', 'GAM_NAME','AGENT_JOIN_YEAR','REGION_NAME'])[['AFYP_Regular', 'AFYP_exclude_RTU','ROP','Cases','Securelink','Megaplus','Maxipro','Others']].sum()
print(ag22_group.shape)

ag22_group

#convert groupby table to DataFrame, safe as df6_22
df6_22 = ag22_group.reset_index()
print(df6_22.shape)

print(df6_22.dtypes)

# add one line on Region
df6_22['Region'] = df6_22['REGION_NAME']

# replace Region with indicator

df6_22['Region'] = df6_22['Region'].replace(['KL REGION', 'CENTRAL REGION','NORTHERN REGION','EAST COAST REGION','SARAWAK REGION','ETIQA ENTREPRENEURSHIP PROGRAM-EEP REGION REGION','TIED AGENCY - ETIQA LIFE INSURANCE','SOUTHERN REGION','SABAH REGION'], ['KL', 'CENTRAL','NORTHERN','EAST COAST','SARAWAK','HQ','HQ','SOUTHERN','SABAH'])

"""**✅ EDA to do on Year 2022 data**"""

# Check agent with min & max production in Year 2022

print("agent with min AFYP in 2022 =", df6_22["AFYP_Regular"].min())
print("agent with max AFYP in 2022 =", df6_22["AFYP_Regular"].max())
print("agent with average AFYP in 2022 =", df6_22["AFYP_Regular"].mean())

print("agent with min cases in 2022 =", df6_22["Cases"].min())
print("agent with max cases in 2022 =", df6_22["Cases"].max())
print("agent with average cases in 2022 =", df6_22["Cases"].mean())

# add on ACS into 2022 data (df6_22)

df6_22['Average_Case_Size'] = df6_22['AFYP_Regular'].div(df6_22['Cases'])
df6_22

# Plot box plot for ag22_group (AFYP)

import seaborn as sns
sns.boxplot('AFYP_Regular', data=df6_22)

# Plot box plot for ag22_group (CASES)

import seaborn as sns
sns.boxplot('Cases',data=df6_22)

# Plot box plot for ag22_group (ACS)

import seaborn as sns
sns.boxplot('Average_Case_Size',data=df6_22)

"""**⏰ Check outlier use AFYP_Regular**"""

# Check for outliers in ag22_group, using IQR

import numpy as np

q1 = np.percentile(df6_22['AFYP_Regular'], 25)
q2 = np.percentile(df6_22['AFYP_Regular'], 50)
q3 = np.percentile(df6_22['AFYP_Regular'], 75)
iqr = q3 - q1
print(q1, q3, iqr)

UL = q3 + 1.5*iqr
LL = q1 - 1.5*iqr

print("U Limit =", UL)
print("L Limit =", LL)

# Check how many outliers, only UL have outliers, no outlier in LL (based on AFYP_Regular)

upper = df6_22['AFYP_Regular'] >= UL
ex = np.where(upper)
print(ex)

lower = df6_22['AFYP_Regular'] <= LL
ex1 = np.where(lower)
print(ex1)

df6_22.shape

# Check row with outliers, store dataframe as ag22_outliers

ag22_outliers = df6_22[df6_22['AFYP_Regular'] >= UL]
print(ag22_outliers.head())

print('outliers in 2022 =', ag22_outliers.shape[0])

# Store data exclude outlier in ag22_clean

ag22_clean = df6_22[df6_22['AFYP_Regular'] < UL]
print(ag22_clean.head())

print('non-outliers in 2022 =', ag22_clean.shape[0])

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

sns.scatterplot(data = ag22_clean, x = 'Cases', y = 'AFYP_Regular');

# use the scatterplot function to build the bubble map
sns.scatterplot(data=ag22_clean, x="Cases", y="AFYP_Regular", size="AFYP_Regular", legend=False, sizes=(20, 2000))

sns.scatterplot(data = ag22_outliers, x = 'Cases', y = 'AFYP_Regular');

"""**⏰ Check outlier use Average case size**"""

import numpy as np

xq1 = np.percentile(df6_22['Average_Case_Size'], 25)
xq2 = np.percentile(df6_22['Average_Case_Size'], 50)
xq3 = np.percentile(df6_22['Average_Case_Size'], 75)
xiqr = xq3 - xq1
print(xq1, xq3, xiqr)

xUL = xq3 + 1.5*xiqr
xLL = xq1 - 1.5*xiqr

print("U Limit =", xUL)
print("L Limit =", xLL)

xupper = df6_22['Average_Case_Size'] >= xUL
xex = np.where(xupper)
print(xex)

xlower = df6_22['Average_Case_Size'] <= xLL
xex1 = np.where(xlower)
print(xex1)

# Check row with outliers, store dataframe as df6_22_outlier

df6_22_outlierx = df6_22[df6_22['Average_Case_Size'] >= xUL]
print(df6_22_outlierx.head())

print('outliers in 2022 (using ACS) =', df6_22_outlierx.shape[0])

"""**⏰ Check outlier use NOC**"""

import numpy as np

xxq1 = np.percentile(df6_22['Cases'], 25)
xxq2 = np.percentile(df6_22['Cases'], 50)
xxq3 = np.percentile(df6_22['Cases'], 75)
xxiqr = xxq3 - xxq1
print(xxq1, xxq3, xxiqr)

xxUL = xxq3 + 1.5*xxiqr
xxLL = xxq1 - 1.5*xxiqr

print("U Limit =", xxUL)
print("L Limit =", xxLL)

xxupper = df6_22['Cases'] >= xxUL
xxex = np.where(xxupper)
print(xxex)

xxlower = df6_22['Cases'] <= xxLL
xex1 = np.where(xxlower)
print(xex1)

# Check row with outliers, store dataframe as df6_22_outlier

df6_22_outlierxx = df6_22[df6_22['Cases'] >= xxUL]
print(df6_22_outlierxx.head())

print('outliers in 2022 (using ACS) =', df6_22_outlierxx.shape[0])

# Plot based on overall data set year 2022

sns.scatterplot(data = df6_22, x = 'Cases', y = 'AFYP_Regular')

"""✅ **VISUALISATION:with Sweetviz - YEAR 2022 DATA**"""

!pip install sweetviz

import sweetviz as sv

#analyzing the dataset
lia_report = sv.analyze(ag22_clean)

#display the report
lia_report.show_html('LIA.html')

lia_report.show_notebook()

report_comp=sv.compare([ag22_clean,'Non-Outlier Data'],[ag22_outliers,'Outlier Data']) 
report_comp.show_html('Compare_Results.html')

report_comp.show_notebook()

df6_22.head(3)

"""**⏰ Take out 3 outliers to check clusters**

---


"""

# ex = above 750k (excluded these 3 from analysis)

ex = df6_22.loc[df6_22.AFYP_Regular > 750000, :]
print(ex.shape[0])
ex.head()

# Data without 3 outliers

x1 = df6_22.loc[df6_22.AFYP_Regular <= 750000, :]
x1.shape

"""**⏰ K-means Clustering - using x1 data**"""

# Using K-Means Clustering to check for clusters

import matplotlib.pyplot as plt
import numpy as np

features = ['Cases', 'AFYP_Regular']
X = x1[features]
print(X)

X.shape
X.describe()

plt.scatter(X['Cases'], X['AFYP_Regular']);

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

#plt.scatter(X[0], X[1], c=y_kmeans, s=50, cmap='viridis')
plt.scatter(X['Cases'], X['AFYP_Regular'], c=y_kmeans, s=50, cmap='viridis')

centers = kmeans.cluster_centers_
centers
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);

# Finding optimal value K

from sklearn.metrics import silhouette_score
no_of_clusters = [2, 3, 4, 5, 6]
for n_clusters in no_of_clusters:

	cluster = KMeans(n_clusters = n_clusters)
	cluster_labels = cluster.fit_predict(X)

	# The silhouette_score gives the average value for all the samples (nearer to 1 the better means cluster is cohearent, -1 is bad)
	silhouette_avg = silhouette_score(X, cluster_labels)

	print("For no of clusters =", n_clusters,
		" The average silhouette_score is :", silhouette_avg)

# Finding optimal value K - elbow method using distortions

no_of_clusters = [2, 3, 4, 5, 6]
distortions = []
for k in no_of_clusters:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(X)
    distortions.append(kmeanModel.inertia_)

plt.figure(figsize=(10,6))
plt.plot(no_of_clusters, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

"""**⏰ Zoom into clusters to check characteristics**
 
 * above 310k
 * between 310k to 100k
 * below 100k
"""

# g1 = above 310k

g1 = x1.loc[x1.AFYP_Regular >= 310000, :]
print(g1.shape[0])
g1.head (5)

g1_count = 24 / 412 * 100

print(g1_count)

g1.describe()

# count g1 agent contributes >54 cases a year

g1_count_c = len(g1[g1["Cases"]>= 54])
print('agent contributes more than 54 cases = ', g1_count_c)

# g1 distribution graph

sns.scatterplot(data = g1, x = 'Cases', y = 'AFYP_Regular')

# count g1 by join year

g1_count = len(g1[g1["AGENT_JOIN_YEAR"]>= 2019])
print('agent join year before 2019 = ', g1_count)
print(g1.shape)


g1_count_x = 30 - 24
print('agent join year before 2019 = ', 24 - 18)

# g1 by region

g1.groupby('Region').size().plot(kind = 'pie', autopct = '%.1f%%', label = ' ');

# Plot by Product bar chart with Region & Cases

# Groupby region, product, Cases store as g1_bar
g1_bar = g1.groupby('Region').agg({'Securelink': 'sum', 'Maxipro': 'sum', 'Megaplus': 'sum', 'Others': 'sum'})

#convert groupby table to DataFrame
g1_bar_df = g1_bar.reset_index()

print(g1_bar_df)

g1_bar_df.plot(kind='bar', x='Region', stacked=True, color=['orange', 'skyblue', 'red', 'green'])
plt.xlabel('Region')
plt.ylabel('Cases')

# g2 = above 200k & below 100c

x1 = df6_22.loc[df6_22.AFYP_Regular < 310000, :]

g2 = x1.loc[x1.AFYP_Regular >= 100000, :]

print(g2.shape[0])
g2.head(5)

# g2 distribution graph

sns.scatterplot(data = g2, x = 'Cases', y = 'AFYP_Regular')

# g2 by region

g2.groupby('Region').size().plot(kind = 'pie', autopct = '%.1f%%', label = ' ');

print(len(g2[g2["Region"] == 'KL']))
print(len(g2[g2["Region"] == 'NORTHERN']))
print(len(g2[g2["Region"] == 'CENTRAL']))

# Plot by Product bar chart with Region & Cases

# Groupby region, product, Cases store as g1_bar
g2_bar = g2.groupby('Region').agg({'Securelink': 'sum', 'Maxipro': 'sum', 'Megaplus': 'sum', 'Others': 'sum'})

#convert groupby table to DataFrame
g2_bar_df = g2_bar.reset_index()

print(g2_bar_df)

g2_bar_df.plot(kind='bar', x='Region', stacked=True, color=['orange', 'skyblue', 'red', 'green'])
plt.xlabel('Region')
plt.ylabel('Cases')

# count g2 by join year

g2_count = len(g2[g2["AGENT_JOIN_YEAR"]>= 2019])
print(g2_count)
print(g2.shape)

g2.describe()

# count g2 agent contributes >100 cases a year

g2_count_c = len(g2[g2["Cases"] >= 100])
print('agent contributes more than 51 cases = ', g2_count_c)

# g3 distribution graph

g3 = df6_22.loc[df6_22.AFYP_Regular < 100000, :]

print(g3.shape[0])
g3.head(5)

g3_count_x1 = 338 / 412 * 100

print(g3_count_x1)

# g3 distribution graph

sns.regplot(data = g3, x = 'Cases', y = 'AFYP_Regular')

g3.describe()

# count g2 by join year

g3_count = len(g3[g3["AGENT_JOIN_YEAR"]>= 2019])
print(g3_count)
print(g3.shape)

# g3 by region

g3.groupby('Region').size().plot(kind = 'pie', autopct = '%.1f%%', label = ' ');

# Plot by Product bar chart with Region & Cases

# Groupby region, product, Cases store as g3_bar
g3_bar = g3.groupby('Region').agg({'Securelink': 'sum', 'Maxipro': 'sum', 'Megaplus': 'sum', 'Others': 'sum'})

#convert groupby table to DataFrame
g3_bar_df = g3_bar.reset_index()

print(g3_bar_df)

g3_bar_df.plot(kind='bar', x='Region', stacked=True, color=['orange', 'skyblue', 'red', 'green'])
plt.xlabel('Region')
plt.ylabel('Cases')

# extra graph to double check the g3 trend

extra = df6_22.loc[df6_22.AFYP_Regular < 50000, :]
sns.regplot(data = extra, x = 'Cases', y = 'AFYP_Regular')